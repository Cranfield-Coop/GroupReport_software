from tqdm import tqdm
import torch
import csv
import numpy as np

# Importing modules for data handling and the neural network model
from TurbulenceModel.TurbulenceDataModule import TurbulenceDataModule
from TurbulenceModel.TurbulenceModelPINN import TurbulenceModelPINN

def run_inference(model_checkpoint_path, prediction_dataset_path, prediction_output_path):
    """
    Runs inference on the test dataset using the specified model checkpoint and saves predictions.

    Args:
        model_checkpoint_path (str): Path to the trained model checkpoint.
        prediction_dataset_path (str): Path to the prediction dataset CSV file.
        prediction_output_path (str): Path to save the prediction output CSV file.
    """
    # Initialize the data module with the test dataset path
    data_module = TurbulenceDataModule(
        batch_size=64,  # Adjust based on your system's memory capacity
        num_workers=8,  # Adjust based on your system's capabilities
    )

    # Load the model from the checkpoint
    model = TurbulenceModelPINN.load_from_checkpoint(
        checkpoint_path=model_checkpoint_path,
    )

    # Prepare the data module specifically for testing
    data_module.setup(stage="predict", predict_dataset_path=prediction_dataset_path)

    # Set the model to evaluation mode
    model.eval()

    mse_total_list = []
    rmse_total_list = []
    
    # Open a file to save the predictions
    with open(prediction_output_path, "w", newline="") as file:
        writer = csv.writer(file)
        # Write the CSV header
        writer.writerow(
            [
                "y^+",
                "y/delta",
                "u_tau",
                "nu",
                "Re_tau",
                "u'u'_pred",
                "v'v'_pred",
                "w'w'_pred",
                "u'v'_pred",
                "U_pred",
                "dU/dy_pred",
                "P_pred",
                "k_pred",
            ]
        )

        # Evaluate the model on the test dataset
        with torch.inference_mode():
            for batch in tqdm(
                data_module.predict_dataloader(), desc="Inference in Progress"
            ):
                features = batch["features"]
                outputs = model(features)

                # Check if targets are available before calculating metrics
                if "targets" in batch:
                    mse_total, rmse_total = calculate_metrics(outputs, batch["targets"])
                    mse_total_list.append(mse_total.item())
                    rmse_total_list.append(rmse_total.item())

                # Convert tensors to numpy arrays for easier manipulation and writing
                features_np = features.cpu().numpy()
                outputs_np = outputs.cpu().numpy()

                # Iterate over the batch and write each instance to the CSV
                for feature, output in zip(features_np, outputs_np):
                    # Combine the feature and output arrays for writing
                    row = np.concatenate((feature, output)).tolist()
                    writer.writerow(row)
        
        # Only calculate mean MSE and RMSE if they were computed
        mse_mean = np.mean(mse_total_list) if mse_total_list else None
        rmse_mean = np.mean(rmse_total_list) if rmse_total_list else None
        return mse_mean, rmse_mean

# Calculate metrics function, added a check for availability of 'targets'
def calculate_metrics(predictions, targets):
    mse_total = torch.nn.functional.mse_loss(predictions, targets)
    rmse_total = torch.sqrt(mse_total)
    return mse_total, rmse_total

if __name__ == "__main__":
    # Example usage
    run_inference(
        model_checkpoint_path="epoch=10461-step=188316.ckpt",  # Trained model checkpoint
        prediction_dataset_path="prediction_5200.csv",  # input csv path generated by prepare_prediction_csv method
        prediction_output_path="prediction_5200_output.csv",  # output csv path
    )
